{% extends "layout/base.html" %} {% block content %}
<div class="container">
  <div class="starter-template">
    <img class="center-image" src="static/images/wallpaper.jpg" width="100%">
    <h1>Animatrix</h1>
    <h3>Introduction</h3>
    <p>
      Animatrix, an anime recommender system, aims to provide anime recommendations through the traditional method of collaborative
      filtering (CF). The key idea behind CF is that similar users have similar interests and that similar items will also
      be liked by these same users. Consequently, this method focuses on providing a recommendation based on a user’s past
      behavior and actions. There are two main types of collaborative filtering: memory-based and model-based and our team
      has focused on implementing a variation of these types of filters in order to explore the effectiveness and efficiency
      in providing relevant anime recommendations. Within memory-based algorithms, we implemented neighborhood-based, user-to-user
      top-N, and item-to-item top-N using multiple similarity measures to compute the similarities between animes and users.
      In addition, we have also focused on exploring memory-based algorithms by attempting to implement Naive Bayes and successfully
      implementing SVD. In order to analyze these different implementations, we compared their root mean square error and
      mean absolute error and compare the results along with discuss challenges and future work.
    </p>
    <h3>Motivation</h3>
    <p>
      The anime recommender system is a useful tool for anime fans who would like to expand their taste in anime based on recommendations
      from like-minded people. Like the music and eCommerce industry, the anime industry contains too much content to be
      consumed normally. Currently, MyAnimeList has over a million registered users who discuss anime recommendations on
      forums which tends to get repetitive. A tool that can recommend new anime based on their likes as well as likes of
      other users with similar preferences would create a more effective recommendation system. This would eliminate the
      need of forum threads dedicated to recommendations and provide more customized anime choices to users.
    </p>
    <h3>Related Work</h3>
    <p>Various anime recommenders using different filtering algorithms have been developed in attempts to develop a tool that
      provides a useful tool for anime recommendations. In an open source project created by Mayank Bhatia, the primary goal
      of the system was to compare the results of the recommendations to MyAnimeList recommendations through content and
      collaborative-based filtering using nearest neighbors as the main algorithm [MAYANK BHATIA]. When creating the kNN
      model for content-based filtering, Bhatia used 11 nearest neighbors and limited the dataset based on the categorical
      variables of title and genre, with absolutely no use of ratings from the current user or any other user. He demonstrated
      a positive qualitative result with no error measures to compare the predictions to the actual results. When creating
      the kNN model for collaborative-based filtering, he concluded it an unsuccessful approach through visually displaying
      the results, but no error measures to fully comprehend the comparison with MyAnimeList. </p>
    <p>Another anime tool developed by Tahsin Mayeesha used a content-based filtering through another kNN model [TAHSIN MAYEESHA
      MEDIUM]. Compared to the previously discussed kNN model however, this tool was developed with a different set of features,
      specifically targeting genre, type of anime, anime episodes, and anime ratings. In addition, considering the tool only
      provides the top 5 most similar animes, the number of neighbors used was 6 based on the query. The purpose of this
      tool was to explore this approach without the use of text features, considering recommending a second season of a show
      would be too simplistic [TAHSIN MAYEESHA MEDIUM]. With this setup, the developer demonstrates a positive qualitative
      approach based on her personal opinion that the recommended animes are similar to the query, which is also a limitation
      to the approach since there is no specific error measures or quantitative approach for comparison in the results of
      the query. In addition, this method also takes advantage of the different features used and is able to successfully
      provide recommendations based on the type of anime feature helping to differentiate between the anime movies in the
      dataset and anime shows.</p>
    <p>In addition to content-based filters, collaborative-filtering is another popular approach for recommendation systems.
      There are very few anime tools that have taken into consideration this approach, most specifically the different types
      of collaborative filtering, including memory-based and model-based. An attempt by Adam M in Kaggle’s anime community
      was made to implement a type of user-based collaborative filtering through cosine similarity [ADAM M KAGGLE]. This
      approach took the top most similar users to the current user and used their ratings to provide the predictions and
      recommended anime for the current user compared to the typical approach of finding similar users that have rated the
      same anime and then providing recommendations. Without any measure of error, however, it is difficult to analyze the
      effectiveness of this approach. </p>
    <p>Based on all the approaches taken, there is currently a few methods that have not been explored and measures of error
      have not been provided in any of the methods described in order to understand the efficiency of the system and the
      predictions provided compared to the actual predictions. As a result, our team has developed an anime recommender system
      using an anime database set collected from MyAnimeList that was publicly published. This recommender system takes into
      consideration memory-based and model-based collaborative filtering, the latter which has never been explored before
      as an option for anime recommendations. The memory-based approaches we take into consideration are nearest-neighborhood-based,
      user-to-user-based top-N, and item-to-item-based top-N, while the model-based approaches we consider are a naive-based
      model and singular value decomposition (SVD). Unlike previous work, we also provide quantitative measures, such as
      root-mean-squared-error (RMSE) and mean-absolute-error (MAE) in order to compare the effectiveness of each system.</p>
    <h3>Proposed Solution and Methods</h3>
    <p>In order to develop an anime recommender system, we decided to explore various options in collaborative filtering (CF).
      Before dwelling on our algorithms and the different parameters we took in consideration for each method implemented,
      we decided to only use a subset of our data due to extensive and long computation times, especially considering we
      need a live server for such computations when the recommender system is used. The dataset we used was sourced from
      MyAnimeList and consisted of user preference data from approximately less than 70,000 users and over 12,000 animes.
      There were two different sets, one of which contained the information about anime and the other containing the ratings
      of a specific user to a specific anime that correlated in the anime data set. Before merging both datasets, the ratings
      data set was filtered to only contain ratings greater than 0, meaning that ratings that were 0 or -1 were not considered
      considering they would drastically affect the calculation for the averages and the rating prediction. In addition,
      the data sets were merged based on the common key of anime ids, any “NaN” terms were replaced with zeros, and the only
      columns considered were the user id, anime ids, anime name, and user rating. Lastly, the subset we considered to train
      our models considered approximately 6,000 users due to the high computation times needed (hours at a time) to provide
      error measures and predictions for all the animes and users in this set. However, we kept this consistently across
      all our models in order to provide a proper evaluation and comparison between the different collaborative filters we
      implemented. With this subset, we created a split for training and testing datasets based on 80% of the data being
      for training and leaving 20% of the data for testing.</p>
    <p>Our first approach for CF consisted of memory-based algorithms. This approach is similar to the classic implementation
      of collaborative filtering, where we first compute the user-user similarities based on their ratings, we then isolate
      the ‘k’ most similar users to each target user and then finally predict the ratings of the target users using the ratings
      of the ‘k’ most similar users who had also rated the same item. The first attempt of this method used very few inbuilt
      data types and APIs, this was not efficient or scalable, so we proceeded to using pandas dataframes and the scikit
      surprise library implementation. We used a basic collaborative filtering method which also took into account a baseline
      rating. The formula for the rating is as shown below:</p>
      <img class="center-image" src="static/images/001.png" width="500">
      <p class="text-center"> <i>Figure 1.</i> Caption.</p>
      <br>
    <p>We choose k=200, and min_k=1 where k is the maximum number of neighbours (most similar users) to be considered, and min_k
      is the minimum number of users to be considered when predicting the rating of an item i by user u, if the minimum is
      not met, the baseline rating is used by default. We arrived at k=200 through trial and error, ranging the value between
      50 and 1000 to see which yielded the best RMSE value. The similarity function used is the pearson baseline similarity,
      the formula is as shown below.</p>
    <img class="center-image" src="static/images/002.png" width="500">
    <p class="text-center"> <i> Figure 2.</i> Caption.</p>
    <br>
    <p>This calculation subtracts the ratings with the baseline rating of the corresponding user and not the user’s average
      rating. </p>
    <p>Another consideration when looking at the similarity was shrinkage, this would allow us to avoid overfitting if too few
      ratings are available. We set the shrinkage parameter to 100 (default), the modified pearson similarity is as follows:</p>
    <img class="center-image" src="static/images/003.png" width="500">
    <p class="text-center"> <i> Figure 3.</i> Caption.</p>
    <br>
    <p>There were two methods considered for calculating the baseline rating; stochastic gradient descent (SGD) and alternating
      least squares (ALS). We chose SGD as it gave us better results. </p>
    <img class="center-image" src="static/images/004.png" width="500">
    <p class="text-center"> <i> Figure 4.</i> Caption.</p>
    <br>
    <p>Here n_epochs corresponds to the number of iterations the SGD runs for, 50 was arrived at after trial and error for the
      best results and learning_rate corresponds to the 𝛾 as seen in [Kor10] which is the learning rate of the SGD, the
      value 0.005 was arrived at after trial and error. </p>
    <p>We changed the -1 ratings to 5 (in a scale of 1-10) because the user went out of his/her way to mark a particular anime
      as watched and so even if they did not give it an explicit rating, we took this to mean that the user was interested
      in the anime, this method also showed improvements in our RMSE value. We will discuss the final RMSE value obtained
      in the Evaluation section. (RMSE: 1.2062 for the first 6000 users)</p>
    <p>Another method in memory-based CF was user-to-user top-N recommendations. This method would compute the similarity between
      users and select the top N users that are most similar to the current user who rated the same anime item. A few aspects
      need to be taken in consideration when using this method, including the similarity computation, the average ratings,
      and the number n for top-N users. Considering there are various similarity computational approaches, we decided to
      implement two of the most popular measures, Pearson correlation and cosine-based similarity. We decided to consider
      both of these similarity measures considering some of the ratings are missing and not all users have rated the same
      item, which means that cosine similarity will consider a non-existing rating as negative or zero, compared to Pearson
      correlation where the rating is not considered in the calculation, taking advantage of an item being rated by both
      users. This slight difference between the computation could provide significant differences in the error measures depending
      on how the data is extracted and normalized. In addition, due to the heavy computations to calculate a rating prediction,
      various aspects of the prediction were pre-computed in order to reduce the amount of time. A similarity matrix was
      created between all the users for cosine and pearson similarity. An average rating matrix was also necessary for each
      of the users based on their total number of ratings and the rating values they provided for that anime. Average rating
      matrices were also created for the training and test datasets that were initially created. In the prediction function,
      we decided on the n-value for the top-N similar users to be 50 considering the subset data size we used consisted of
      approximately 6,000 users. Based on the equation we used to calculate the aggregation of similar users of a specific
      item, as shown in [FIGURE BLAH], a user was only selected to be similar to the current user if the item was rated by
      both users or else only the current user’s average rating was returned. The k value in [FIGURE BLAH] is the normalizing
      factor and is calculated by taking the weighted average of the similarity between two users. As demonstrated in [FIGURE
      2 BLAH], the implementation of a cosine user to user rating prediction is able to provide a rating prediction for a
      particular user, in this case user with ID 3, by taking in consideration the cosine similarity and returning a rating
      of 6.607 for that particular anime “Zombie-Loan”. The same type of implementation was used for the pearson similarity
      by using a covariance through Numpy’s library. In order to further evaluate the results and the accuracy of the prediction,
      the mean absolute error (MAE) and the root mean square error (RMSE) were calculated for all the users in the test data
      with the anime data for that specific user, which is further discussed in the Evaluation section. </p>
    <img class="center-image" src="static/images/005.png" width="500">
    <p class="text-center"> <i>Figure 5.</i> The rating for user u and item i, where u’ is the similar user and ru is the average ratings of user
      u for all the items rated by user u.</p>
      <br>
    <img class="center-image" src="static/images/006.png" width="500">
    <p class="text-center"> <i>Figure 6.</i> User-to-User Top-N rating prediction implementation using cosine similarity and weighted averages as
      the normalizing factor.</p>
      <br>
    <p>In the item-to-item top-N recommendations, a similar method was used in comparison with user-to-user top-N recommendations,
      as shown in [FIGURE BLAH]. The similarity measures considered were also cosine and pearson, the average ratings for
      each item were pre-computed as well for the training and testing data sets, and the top-N items considered was also
      50 considering the same subset of data was used. Unlike the user-to-user equation implemented, this algorithm focused
      on predicting the rating for an anime for a particular user based on the anime ratings of similar anime items. As a
      result, the main difference between the user-to-user implementation and item-to-item implementation was figuring out
      the similarity between items, the average rating of that specific item from all users, and calculating the prediction
      based on the top 50 similar items. In addition, the MAE and RMSE were also calculated for all the items in the test
      data in order further ealue the results and accuracy of the prediction, which is further discussed in the Evaluation
      section.</p>
    <img class="center-image" src="static/images/007.png" width="500">
    <p class="text-center"> <i>Figure 7.</i> Item-to-Item Top-N rating prediction implementation using pearson similarity and weighted averages as
      the normalizing factor.</p>
      <br>
    <p>In addition to these various memory-based algorithms, we also considered model-based CF approaches. The first model we
      considered was the Naive Bayes recommender model (not to be confused with the similar Bayesian Networks model), in
      part because of the strong success of the Naive Bayes classifier that we used to classify texts. Naive Bayes recommenders
      make the naive assumption that each “feature” is totally independent (much like the Naive Bayes classifiers). In the
      recommendation context, each user’s opinion could be considered an independent “feature” of the item. Extending the
      classifier further, the recommender attempts to “classify” each item into one of several classes corresponding to each
      possible rating (e.g. “Class 5/5”, “Class 4/5", and so on).</p>
    <img class="center-image" src="static/images/008.png" width="500">
    <p class="text-center"> <i>Figure 8.</i> A formula for the basic Naive Bayes implementation [Breese]. On the left is Probability (Class of item
      = a given class GIVEN v1 to vn, each user’s vote). Using Bayes’s Formula to reverse the conditionals, we have the formula
      on the right.</p>
      <br>
    <img class="center-image" src="static/images/009.png" width="500">
    <p class="text-center"> <i>Figure 9.</i> Another formulation for Naive Bayes [Pazzani]. The right-hand side of this equation mirrors the equation
      above. It illustrates one simple way to predict a rating: take the rating-class with the highest probability.</p>
      <br>
      <p>Several challenges arose while attempting to design and implement the Naive Bayes model. The first was that the Naive
      Bayes recommender model was less discussed than some other popular models -- some standard equations were given in
      research papers, but the models were discussed tersely, with few examples given. The second is that there were a number
      of ways to extend the model, and these extensions were the primary discussion of the literature found. </p>
    <p>On the implementation side, another issue is that the right formula requires calculating Pr(C=c) and Pr(vi | C=c), which
      can be estimated from the training set of user votes, but with some difficulty in implementation. There was some trouble
      figuring out the calculations; attempts didn’t vectorize cleanly over the Pandas DataFrames, and using nested for-loops
      is messy and slow. On the other hand, using scikit-learn’s built-in Naive Bayes methods also failed, due to the sparsity
      and incompleteness of the user-rating matrix (scikit-learn’s built-in requires all data/all features be present). datascience.stackexchange.com/questions/13597/
      [Possibly remove this link later]</p>
    <p>With a struggle to get results with Naive Bayes, we decided to switch to a different model-based method. SVD with low-rank
      decomposition was known to be effective, and was a familiar model in the literature. In addition, there were several
      out-of-box solutions for matrix decomposition. Implementing the SVD recommender model came down to choosing which solution
      to use, and building around it. </p>
    <img class="center-image" src="static/images/010.png" width="500">
    <p class="text-center"> <i>Figure 10.</i> The basic principle behind SVD. The idea is to decompose the user-rating matrix A into three matrices.
      Each “latent factor” produced by this decomposition represents some “concept”--perhaps genre, creator, or something
      completely foreign to human intuition. The matrix sigma represents the strength of each latent factor. U is a matrix
      of (number of users rows) by (number of factors columns), representing the user-concept space (which users like which
      concepts). VT is a matrix of (number of factors) by (number of items), representing the item-concept space (which items
      are associated with which concepts).</p>
      <br>
    <p>At first, we considered methods from scipy (linalg.SVDs) and sklearn (TruncatedSVD). These each had their own issues
      -- linalg.SVDs did the U, sigma, VT decomposition discussed in class, but as discussed in class, that decomposition
      can struggle with missing values. In addition, both linalg.SVDs and TruncatedSVD lacked methods for specific predictions--that
      is, generating predictions for a specified user and item(s). This was a problem because our demo required exactly that
      to be done. These methods could generate predictions for the entire matrix, but this was costly; in general, multiplying
      the factored matrices took too much time and memory (as matrix multiplication for such large matrices is computationally
      expensive). </p>
    <p>The SVD method provided by scikit-surprise proved more fruitful, as it was designed specifically for the content-recommendation
      context (and not more general contexts, as scipy and sklearn may be). It implemented the SVD algorithm as described
      by Simon Funk and refined by BellKor, who first popularized it during the Netflix Prize competition -- using baselines,
      biases, regularization, and gradient descent. </p>
    <img class="center-image" src="static/images/011.png" width="400">
    <p class="text-center"> <i>Figure 11.</i> The formula for calculating a predicted rating, for user u and item i. Mu represents the global average
      rating; bu represents the bias for each user (if a user tends to rate higher or lower than the global average), and
      bi represents the bias for each item (if an item tends to be higher or lower than the global average). qiTpu represents
      the matrix multiplication seen earlier, multiplying the item-concept matrix Q by the concept-user matrix P. If an item
      or user is missing, the bias and matrix multiplication are assumed to be zero. </i>
    </p>      <br>
    <img class="center-image" src="static/images/012.png" width="500">
    <p class="text-center"> <i>Figure 12.</i> The error formula. This error formula measures how far off our q, p, item bias, user bias, and regularization
      terms are, in order to tune them to be better. To minimize the total error, each parameter is subject to stochastic
      gradient descent.</p>
      <br>
    <p>To quote Simon Funk:
      <i>“Only problem is, we don't have 8.5B entries, we have 100M entries and 8.4B empty cells. ...But, just because there
        are five hundred complicated ways of computing singular value decompositions in the literature doesn't mean there
        isn't a really simple way too: Just take the derivative of the approximation error and follow it. This has the added
        bonus that we can choose to simply ignore the unknown error on the 8.4B empty slots.”</i> This had a lot of power
      and customization. At first, the default parameters were selected under the assumption that they were reasonable defaults
      -- these parameters were also based on Simon Funk’s discussions. These results were acceptable, but we found slight
      improvement by tweaking the parameters slightly. We used 50 latent factors (as opposed to the default 100), and a regularization
      multiplier of 0.05 (as opposed to the default 0.02). </p>
    <h3>Evaluation and Analysis of Results</h3>
    <p>In order to evaluate the rating predictions, we used two different measures: root mean square error (RMSE) and mean absolute
      error (MAE). These two measures are good for analysis of the different algorithms in order to minimize the loss of
      the function. A loss function will measure the distance between the predicted ratings, in this case, the actual ratings.
      This distance can be defined and measured in various ways, but the main goal is to try to achieve a minimal loss in
      order to make a prediction as close as possible to the actual rating. RMSE is a measure to take into consideration
      considering larger magnitudes will make a larger impact on the result considering the error terms are squared. This
      will allow us to understand the effect large error terms have compared to smaller error terms. However, this could
      also be hindrance in the measure considering outliers drastically affect the final result and it also varies with strong
      variability with the distribution of error magnitudes. MAE is another measure we take into consideration since it tries
      to resolve the issues mentioned by taking the absolute value of the prediction and actual rating causing it to be less
      sensitive to outliers along with holding steady results despite the increase in variance.</p>
    <table class="table table-bordered text-center">
      <thead class="text-center">
        <tr>
          <th colspan="3" class="text-center">
            <b>
              <i>Accuracy Measures for Collaborative Filtering Methods</i>
            </b>
          </th>
        </tr>
        <tr>
          <th class="text-center">
            <b>
              <i>Type of Collaborative Filtering</i>
            </b>
          </th>
          <th class="text-center">
            <b>
              <i>MAE</i>
            </b>
          </th>
          <th class="text-center">
            <b>
              <i>RMSE</i>
            </b>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>k-Nearest Neighbors</td>
          <td>0.8878</td>
          <td>1.206</td>
        </tr>
        <tr>
          <td>User-User Top-N Cosine Similarity</td>
          <td>0.8976</td>
          <td>1.182</td>
        </tr>
        <tr>
          <td>User-User  Top-N Pearson Similarity</td>
          <td>0.9086</td>
          <td>1.195</td>
        </tr>
        <tr>
          <td>Item-Item Top-N Cosine Similarity</td>
          <td>0.8696</td>
          <td>1.154</td>
        </tr>
        <tr>
          <td>Item-Item Top-N Pearson Similarity</td>
          <td>0.8654</td>
          <td>1.153</td>
        </tr>
        <tr>
          <td>SVD (50 latent factors, regularization = 0.05)</td>
          <td>0.8886</td>
          <td>1.202</td>
        </tr>
      </tbody>
    </table>
    <p class="text-center"> <i>Table 1.</i> Caption</p>
    <br>
    <table class="table table-bordered text-center">
      <thead class="text-center">
        <tr>
          <th colspan="2" class="text-center">
            <i>Execution Time Frame for Collaborative Filtering Methods</i>
        </tr>
        <tr>
          <th class="text-center">
            <b>
              <i>
                Type of Collaborative Filtering
              </i>
            </b>
          </th>
          <th class="text-center">
            <b>
              <i>
                Total Duration (Training and Prediction Evaluation)
              </i>
            </b>
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            k-Nearest Neighbors
          </td>
          <td>
            ~ 3 min. 35 sec.
          </td>
        </tr>
        <tr>
          <td>
            User-User Top-N Cosine Similarity
          </td>
          <td>
            ~ 2 hrs. 45 min.
          </td>
        </tr>
        <tr>
          <td>
            User-User Top-N Pearson Similarity
          </td>
          <td>
            ~ 2 hrs. 45 min.
          </td>
        </tr>
        <tr>
          <td>
            Item-Item Top-N Cosine Similarity
          </td>
          <td>
            ~ 6 hrs. 30 min.
          </td>
        </tr>
        <tr>
          <td>
            Item-Item Top-N Pearson Similarity
          </td>
          <td>
            ~ 6 hrs. 30 min.
          </td>
        </tr>
        <tr>
          <td>
            SVD
          </td>
          <td>
            ~ 1 min. 20 sec.
          </td>
        </tr>
      </tbody>
    </table>
    <p class="text-center"> <i>Table 2. </i> Summary of the MAE and RMSE values for the different type of collaborative filters implemented </p>
    <br>
    <p>Through the subset of data we used, the kNN implementation showed a 1.206 RMSE value and a 0.8878 MAE value. A value closer to zero would be an ideal implementation considering it will represent zero error and loss. However, considering rates range from from 1 to 10, these results provide a baseline of comparison with the other methods that were implemented. The user-to-user top-N approach had two measures of similarity, where cosine similarity demonstrated a better MAE and RMSE than pearson similarity, with values 0.8976 and 1.182, respectively, in comparison to pearson’s similarity values of 0.9086 for MAE and 1.195 for RMSE. However, when comparing this approach to that of kNN, it becomes apparent that the MAE value is lower while cosine similarity of user to user is lower than any of these three approaches. Considering only these methods, for the moment, it can be noted that the high RMSE value in kNN could be caused by outliers in the data set causing a larger error for specific terms. This can be caused due to having a specific set of neighbors and not necessarily being strongly correlated to these neighbors. However, considering the MAE is the absolute value of each of the terms, it is able to eliminate this large outlier impact and surprising maintain the lowest MAE value. </p>
    <p>Unlike user-to-user approach, the item-to-item top-N approach instead provides a lower MAE and RMSE for pearson similarity compared to cosine similarity. The pearson similarity demonstrates 0.8654 for MAE and 1.153 for RMSE while the cosine similarity demonstrates 0.8696 for MAE and 1.154 for RMSE. Taking into consideration all these memory-based methods, it becomes evident that the item-to item top-n approach provided the best accuracy results with the subdataset used, especially using pearson similarity as the similarity weight. However, it is important to note that despite these accuracy measures, the amount of time that it takes to provide these type of computations is unreasonably and unrealistically long. The item-to-item approaches each took approximately six hours compared to the user-to-user approaches that each took approximately two hours to finish computing on the subdataset despite having pre-computed various aspects of the equation. As a result, both of these approaches prove to be unrealistic in providing anime recommendations for users despite their accuracy. </p>
    <p>Lastly, when we considered the model-based collaborative filtering algorithm, SVD, it was interesting to observe an MAE value of 0.8886 and an RMSE value of 1.2022. It is surprising to realize that this method ranks fourth in MAE values and fifth-best in RMSE efficiency compared to the other collaborative filtering methods. One possible cause for the higher error is that the algorithm was trained on a specific subset of the users (as using the entire dataset would have been too computationally expensive); with a smaller amount of data, each mistake is more pronounced. Another possible cause is that the SVD algorithm overfit the training data (and did not generalize to the test data). To combat this we changed the model to be more general (decreasing the number of latent factors from the default 100 to 50, and increasing the regularization parameter from the default 0.02 to 0.05. A summary of the final accuracy results on the subset data can be seen in [FIGURE BLAH/TABLE]</p>
    <p>After careful deliberation, our recommender system was implemented with the SVD algorithm. We could have easily taken into consideration only the accuracy results for our recommender system, but the purpose of Animatrix was to be able to provide anime fans with a new recommender tool that did not only consider the content of the anime as part of the recommendation considering it is easy for MyAnimeList to simply recommend the second season of the same show a user is watching as a recommendation. As a result, we wanted to provide a useful tool that would realistically be used by anime fans, and therefore, accuracy and response time were both measures to be taken into consideration. Due to the fact that SVD provided the best response time, especially after training the data set, we decided that our recommender system would be especially useful with a fast response time. In addition, in order to provide recommendations to a random user, we ask for the user to simply input their feedback on a small set of anime. We use this feedback in order to train the model and provide the predictions of animes based on this input. Afterwards, the top 55 animes are returned to the user as recommended items based on the highest predictions provided from the SVD algorithm.</p>
    <a href="static/images/rate-animatrix.png" target="_blank"><img class="center-image" src="static/images/rate-animatrix.png" width="500"></a>
    <p class="text-center"> <i>Figure 13.</i> Caption. </p> 
    <a href="static/images/ratings-animatrix.png" target="_blank"><img class="center-image" src="static/images/ratings-animatrix.png" width="500"></a>
    <p class="text-center"> <i>Figure 14.</i> Caption. </p> 
    <h3>Conclusions and Discussion</h3>
  </div>
</div>
{% endblock %} {% block footer %} {% include "include/footer.html" %} {% endblock %}